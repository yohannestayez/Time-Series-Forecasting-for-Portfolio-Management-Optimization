
# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from pmdarima import auto_arima
from sklearn.metrics import mean_absolute_error, mean_squared_error
from statsmodels.tsa.stattools import adfuller
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import MinMaxScaler
import os


def preprocess_data(data):
    """
    Preprocesses the given dataset to clean data, calculate features like returns and volatility, 
    and create lagged features for time-series analysis.

    Args:
        data (pd.DataFrame): Input DataFrame containing stock or financial data. 
                             Expected to have at least an 'Adj Close' column.

    Returns:
        pd.DataFrame: Processed DataFrame with additional calculated features and no missing values.
    """
    # Ensure 'Adj Close' column is numeric; if conversion fails, replace invalid entries with NaN
    data['Adj Close'] = pd.to_numeric(data['Adj Close'], errors='coerce')
    
    # Calculate daily percentage returns from the 'Adj Close' column
    data['Return'] = data['Adj Close'].pct_change()
    
    # Calculate rolling volatility (standard deviation of returns) over a 30-day window
    data['Volatility'] = data['Return'].rolling(window=30).std()
    
    # Calculate the rolling mean (30-day moving average) of 'Adj Close'
    data['Rolling_Mean'] = data['Adj Close'].rolling(window=30).mean()
    
    # Create lagged features for the 'Adj Close' column, shifting by 1, 2, and 3 days
    lags = [1, 2, 3]
    for lag in lags:
        data[f'Lag_{lag}'] = data['Adj Close'].shift(lag)
    
    # Remove rows with missing values generated by rolling calculations and lagging
    data.dropna(inplace=True)
    
    # Ensure the DataFrame is sorted by its index for consistent analysis
    data = data.sort_index()
    
    # Return the cleaned and enriched dataset
    return data

# Function to check the stationarity of a time series using the Augmented Dickey-Fuller (ADF) test
def check_stationarity(data, significance_level=0.05):
    """
    Checks the stationarity of a time series using the Augmented Dickey-Fuller (ADF) test.
    
    Args:
        data (array-like): The time series data to be tested for stationarity.
        significance_level (float, optional): The threshold for the p-value to determine stationarity. 
                                              Default is 0.05 (5% significance level).
    
    Returns:
        bool: True if the series is stationary (p-value < significance_level), False otherwise.
    """
    # Perform the Augmented Dickey-Fuller test on the input data
    result = adfuller(data)
    
    # Extract the p-value from the test results
    p_value = result[1]
    
    # Determine if the time series is stationary based on the p-value
    is_stationary = p_value < significance_level
    
    # Print the test result, showing the p-value and whether the series is stationary
    print(f"ADF Test p-value: {p_value:.5f} | Stationary: {is_stationary}")
    
    # Return the stationarity status (True if stationary, False otherwise)
    return is_stationary
# Function to apply differencing to a time series until it becomes stationary
def make_stationary(data, max_diffs=3):
    """
    Transforms a non-stationary time series into a stationary one by applying differencing.
    
    Args:
        data (pd.Series or array-like): The time series data to be made stationary.
        max_diffs (int, optional): Maximum number of differencing operations to apply. Default is 3.
    
    Returns:
        tuple: 
            - pd.Series or array-like: The transformed stationary series (if achieved within max_diffs).
            - int: The number of differencing operations applied.
    """
    # Initialize a counter to track the number of differencing operations
    diff_count = 0
    
    # Keep applying differencing until the series becomes stationary or max_diffs is reached
    while diff_count < max_diffs:
        # Check if the series is stationary using the ADF test
        if check_stationarity(data):
            # Break the loop if the series is stationary
            break
        
        # Apply differencing and drop NaN values generated by differencing
        data = data.diff().dropna()
        
        # Increment the counter for the number of differencing operations
        diff_count += 1
        
        # Print the current progress of differencing
        print(f"Applied differencing {diff_count} time(s)")
    
    # Return the stationary series (if achieved) and the number of differencing operations
    return data, diff_count


# Function for training ARIMA models with stationarity handling
def train_arima_models(data, model_name):
    """
    Train ARIMA and SARIMA models on a given time series data.

    Parameters:
    - data: DataFrame containing time series data with 'Adj Close' column.
    - model_name: Name to be used for saving the models.

    Returns:
    - arima_forecast: Forecasted values from the ARIMA model.
    - sarima_forecast: Forecasted values from the SARIMA model.
    - test: Actual test values for comparison.
    """
    # Use the adjusted close price for forecasting
    series = data['Adj Close']
    
    # Ensure stationarity by differencing if necessary
    stationary_series, num_diffs = make_stationary(series)
    
    # Split the stationary series into train and test sets
    train_size = int(len(stationary_series) * 0.8)
    train, test = stationary_series[:train_size], stationary_series[train_size:]
    
    # Find the best ARIMA parameters using auto_arima
    print("Tuning ARIMA parameters using auto_arima...")
    arima_model = auto_arima(train, seasonal=False, trace=True, d=num_diffs, suppress_warnings=True)
    arima_order = arima_model.order
    print(f"Best ARIMA order: {arima_order}")
    
    # Train ARIMA model
    print("Training ARIMA model...")
    arima_result = ARIMA(train, order=arima_order).fit()
    arima_forecast = arima_result.forecast(steps=len(test))
    
    # Save ARIMA model
    arima_result.save(f'../models/{model_name}_arima_model.pkl')

    # SARIMA model with seasonal component if needed
    print("Training SARIMA model...")
    seasonal_order = (1, 1, 1, 12)  # Adjust seasonal order if needed
    sarima_model = SARIMAX(train, order=arima_order, seasonal_order=seasonal_order)
    sarima_result = sarima_model.fit(disp=False)
    sarima_forecast = sarima_result.forecast(steps=len(test))
    
    # Save SARIMA model
    sarima_result.save(f'../models/{model_name}_sarima_model.pkl')
       
    # Calculating MAE, RMSE, and MAPE for ARIMA
    arima_mae = mean_absolute_error(test, arima_forecast)
    arima_rmse = np.sqrt(mean_squared_error(test, arima_forecast))
    arima_mape = np.mean(np.abs((test - arima_forecast) / test)) * 100

    # Calculating MAE, RMSE, and MAPE for SARIMA
    sarima_mae = mean_absolute_error(test, sarima_forecast)
    sarima_rmse = np.sqrt(mean_squared_error(test, sarima_forecast))
    sarima_mape = np.mean(np.abs((test - sarima_forecast) / test)) * 100

    # Print results
    print("\nEvaluation Metrics:")
    print(f"ARIMA MAE: {arima_mae:.4f}")
    print(f"ARIMA RMSE: {arima_rmse:.4f}")
    print(f"ARIMA MAPE: {arima_mape:.2f}%\n")

    print(f"SARIMA MAE: {sarima_mae:.4f}")
    print(f"SARIMA RMSE: {sarima_rmse:.4f}")
    print(f"SARIMA MAPE: {sarima_mape:.2f}%")
        
    return arima_forecast, sarima_forecast, test
# Define a modified LSTM model for time-series prediction
class LSTMModel(nn.Module):
    """
    A PyTorch implementation of an LSTM model for time-series prediction.
    
    Args:
        input_size (int): The number of features in the input data. Default is 1 (univariate time series).
        hidden_layer_size (int): The number of features in the hidden state of the LSTM. Default is 50.
        output_size (int): The size of the output layer. Default is 1 (single value prediction).

    Attributes:
        hidden_layer_size (int): Stores the size of the LSTM's hidden layer.
        lstm (nn.LSTM): The LSTM layer to process sequential data.
        linear (nn.Linear): The linear layer to map the LSTM's output to the desired output size.
    """
    def __init__(self, input_size=1, hidden_layer_size=50, output_size=1):
        super(LSTMModel, self).__init__()
        self.input_size = input_size  
        # Store the size of the hidden layer
        self.hidden_layer_size = hidden_layer_size
        
        # Define the LSTM layer: input size and hidden layer size
        # batch_first=True indicates that the input tensor is shaped as (batch_size, sequence_length, input_size)
        self.lstm = nn.LSTM(input_size, hidden_layer_size, batch_first=True)
        
        # Define a fully connected (linear) layer to map the LSTM output to the target output size
        self.linear = nn.Linear(hidden_layer_size, output_size)

    def forward(self, input_seq):
        """
        Defines the forward pass of the model.
        
        Args:
            input_seq (torch.Tensor): The input sequence with shape (batch_size, sequence_length, input_size).
        
        Returns:
            torch.Tensor: The model's predictions with shape (batch_size, output_size).
        """
        # Pass the input sequence through the LSTM layer
        # lstm_out contains the output of the LSTM at all time steps
        # The second output (_) is the hidden state and cell state, which are not used here
        lstm_out, _ = self.lstm(input_seq)
        
        # Extract the output of the last time step from the LSTM's output
        # lstm_out[:, -1, :] selects the output for the last time step for all sequences in the batch
        last_time_step_output = lstm_out[:, -1, :]
        
        # Pass the last time step's output through the linear layer to get predictions
        predictions = self.linear(last_time_step_output)
        
        # Return the predictions
        return predictions

    
# Function for training and evaluating an LSTM model
def train_lstm_model(data, feature_cols, model_name):
    """
    Trains an LSTM model for time-series forecasting and evaluates its performance.
    
    Parameters:
        data (DataFrame): Input data containing features and target columns.
        feature_cols (list): List of column names to be used as input features.
        model_name (str): Name to save the trained LSTM model.
    
    Returns:
        model (LSTMModel): The trained LSTM model.
        y_pred (ndarray): Predicted values for the test dataset.
        lstm_mae (float): Mean Absolute Error of the predictions.
        lstm_rmse (float): Root Mean Squared Error of the predictions.
    """
    # Scale the features and target values to the range (-1, 1)
    feature_scaler = MinMaxScaler(feature_range=(-1, 1))
    target_scaler = MinMaxScaler(feature_range=(-1, 1))

    # Apply feature scaling to input features
    data[feature_cols] = feature_scaler.fit_transform(data[feature_cols])
    # Apply feature scaling to the target column 'Adj Close'
    data['Adj Close'] = target_scaler.fit_transform(data[['Adj Close']])

    # Define the sequence length for the LSTM model
    sequence_length = 30
    # Split the data into training (80%) and testing (20%) sets
    train_size = int(len(data) * 0.8)
    train_data = data[:train_size]
    test_data = data[train_size:]
    
    # Helper function to create sequences for LSTM input
    def create_sequences(data, sequence_length):
        """
        Creates input-output pairs for training/testing the LSTM model.
        
        Parameters:
            data (DataFrame): Data to process.
            sequence_length (int): Number of time steps in each sequence.
        
        Returns:
            xs (ndarray): Input sequences.
            ys (ndarray): Corresponding target values.
        """
        xs, ys = [], []
        for i in range(len(data) - sequence_length):
            # Extract a sequence of input features
            x = data.iloc[i:i+sequence_length][feature_cols].values
            # Extract the corresponding target value
            y = data.iloc[i+sequence_length]['Adj Close']
            xs.append(x)
            ys.append(y)
        return np.array(xs), np.array(ys)
    
    # Generate input-output pairs for training and testing
    X_train, y_train = create_sequences(train_data, sequence_length)
    X_test, y_test = create_sequences(test_data, sequence_length)
    
    # Create a DataLoader for training
    train_loader = DataLoader(TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train)), batch_size=64, shuffle=True)
    
    # Initialize the LSTM model
    model = LSTMModel(input_size=len(feature_cols))
    # Define the loss function (Mean Squared Error)
    loss_function = nn.MSELoss()
    # Define the optimizer (Adam optimizer with learning rate 0.001)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # Train the LSTM model
    epochs = 30
    for epoch in range(epochs):
        model.train()  # Set the model to training mode
        for seq, labels in train_loader:
            optimizer.zero_grad()  # Clear previous gradients
            y_pred = model(seq)  # Forward pass
            # Compute the loss between predicted and true values
            loss = loss_function(y_pred, labels.view(-1, 1))  # Reshape labels for compatibility
            loss.backward()  # Backpropagation
            optimizer.step()  # Update model parameters
        # Print the loss for each epoch
        print(f"Epoch {epoch+1}/{epochs} Loss: {loss.item():.4f}")
    
    # Save the trained LSTM model
    torch.save(model, f'../models/{model_name}_lstm_model.pth')
    print(f"Model saved as '{model_name}_lstm_model.pth'")

    # Evaluate the trained model on the test set
    model.eval()  # Set the model to evaluation mode
    with torch.no_grad():  # Disable gradient computation
        # Predict the target values for the test set
        test_inputs = torch.Tensor(X_test)
        y_pred = model(test_inputs).numpy()
    
    # Inverse scale the predictions and true values to original scale
    y_pred = target_scaler.inverse_transform(y_pred)
    y_test = target_scaler.inverse_transform(y_test.reshape(-1, 1))  # Reshape y_test for compatibility
    
    # Calculate evaluation metrics
    lstm_mae = mean_absolute_error(y_test, y_pred)  # Mean Absolute Error
    lstm_rmse = np.sqrt(mean_squared_error(y_test, y_pred))  # Root Mean Squared Error
    # Mean Absolute Percentage Error
    lstm_mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  
    
    # Print evaluation results
    print(f"LSTM MAE: {lstm_mae:.4f}")
    print(f"LSTM RMSE: {lstm_rmse:.4f}")
    print(f"LSTM MAPE: {lstm_mape:.2f}%")
    
    # Return the trained model, predictions, and metrics
    return model, y_pred, lstm_mae, lstm_rmse
